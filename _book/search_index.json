[
["index.html", "Legal Education Analysis in R Chapter 1 About the Book", " Legal Education Analysis in R Richard G. Gardiner 2019-05-15 Chapter 1 About the Book The book was developed as a personal project by the author in an attempt to store important information about different models for legal education. Where possible, I attempt to use real law school data to show the examples. This book is free to use within AccessLex, but I only ask that you give attribution. "],
["intro.html", "Chapter 2 Introduction", " Chapter 2 Introduction This is a placeholder for the introduction "],
["ols.html", "Chapter 3 OLS 3.1 Intuition 3.2 Running your first regression 3.3 Advancing to Multiple Regression 3.4 Checking Assumptions 3.5 Displaying Results", " Chapter 3 OLS OLS regression is the backbone of statistics (though not actually used that often because of the restrictions that come with it). The basic goal of OLS regression is to understand the relationship between one more more independent variables and some dependent variable. OLS regression is said to be BLUE under certain circumastances: B: best L: linear U: unbiased E: estimator Under assumptions that will be discussed later, OLS regression will be unbiased (errors are evenly distributed) and the one with the smallest errors. It is also the best model to use under these assumptions. OLS regression is also very easy to interpret. For all of these reasons (and more), it is one of the first statistical methods taught in graduate methods sections. These do assume, however, that you understand t statistics, standard errors, and the basics of hypothesis testing. 3.1 Intuition Keeping it simple, a bivariate regression looks for the relationship between two variables (sometimes referrred to as vectors). Visually you can see it in the graph below (gapminder library was already loaded). gapminder %&gt;% ggplot(aes(x = gdpPercap, y = lifeExp)) + geom_point() + scale_x_log10(labels = scales::comma_format()) + labs(x = &quot;Logged GDP/capita&quot;, y = &quot;Life Expectancy&quot;) You can see a general positive trend between the logged GDP per Capita and Life expectancy. To see this as a hypothesis, you would say that “countries with higher GPD per capita have a higher life expectancy” or “as a country gets richer, the life expectancy increases.” In the code below, the red line would be considered our hypothesis. gapminder %&gt;% ggplot(aes(x = gdpPercap, y = lifeExp)) + geom_point() + scale_x_log10(labels = scales::comma_format()) + geom_abline(intercept = -10, slope = 20, color = &quot;red&quot;, lty = 2, size = 2) + labs(x = &quot;Logged GDP/capita&quot;, y = &quot;Life Expectancy&quot;) 3.2 Running your first regression Our argument about the connection between logged GDP per captia and life expectancy can be shown in three slightly different ways: Hypothesis: Countries with higher GPD per capita has a higher life expectancy Mathematical: \\(LifeExp_i = \\alpha + \\beta (Logged GDP/Capita_i) + \\epsilon_i\\) R code: model1 &lt;- lm(lifeExp ~ log10(gdpPercap), data = gapminder) The generalizable mathematical formula is actually \\(Y_i = \\alpha + \\beta X_i + \\epsilon_i\\), but I find this to be confusing if introduced before the actual formula. For the R code, the lm stands for linear model. Within the parentheses you need to specify your dependent variable, DV, to the left of the tilde ~ and your independent variable, IV, to the right. Lastly, you need to specify the dataset where the variables are located, data = gapminder (technically you can use the DATASET$VARIABLE notation, but that gets impractical later). Generally, you do not need to worry about the log10(), but prior knowledge says that GDP generally requires a logarithmic transformation. model1 &lt;- lm(lifeExp ~ log10(gdpPercap), data = gapminder) By calling summary(model1) we are able to see the influence of the logged GDP per capita on life expectancy. The first thing we see is the formula we used to create the model. Next we see some summary statistics about our residuals (distance between our regression line and the observation). Under the coefficients section we see our intercept and log10(gdpPercap). The intercept is the location on the y-axis when our independent variable is equal to 0. In many instances, we have no interest in interpreting the intercept. For instance, our model says that when the logged GDP of a country is 0 (which is undefined because logarithm is only for values greater than 0), the life expectancy for that country is -9, another ridiculous number. Again, this is common. Our main interest is in our independent variable, the log of GPD per capita. We see that there is a positive relationship between the log of GPD per capita and life expectancy. Additionally, we see that the t statistic is rather large and the variable is significant at all standard social science levels. The last statistic I want to highlight is the \\(R^2\\) which basically shows us how much of the variation in life expectancy is explained by the log of GDP per capita. summary(model1) ## ## Call: ## lm(formula = lifeExp ~ log10(gdpPercap), data = gapminder) ## ## Residuals: ## Min 1Q Median 3Q Max ## -32.778 -4.204 1.212 4.658 19.285 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -9.1009 1.2277 -7.413 1.93e-13 *** ## log10(gdpPercap) 19.3534 0.3425 56.500 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.62 on 1702 degrees of freedom ## Multiple R-squared: 0.6522, Adjusted R-squared: 0.652 ## F-statistic: 3192 on 1 and 1702 DF, p-value: &lt; 2.2e-16 To get a table more common in articles or presentations, try using the stargazer table from the stargazer package. This table shows the same basic data as the summary with slight stylistic differences. We specified that we wanted the type = &quot;text&quot; because the default is \\(\\LaTeX\\) code. There are a lot of ways to customize a stargazer table, but the basics do most of what we need. stargazer(model1, type = &quot;text&quot;) ## ## =============================================== ## Dependent variable: ## --------------------------- ## lifeExp ## ----------------------------------------------- ## log10(gdpPercap) 19.353*** ## (0.343) ## ## Constant -9.101*** ## (1.228) ## ## ----------------------------------------------- ## Observations 1,704 ## R2 0.652 ## Adjusted R2 0.652 ## Residual Std. Error 7.620 (df = 1702) ## F Statistic 3,192.273*** (df = 1; 1702) ## =============================================== ## Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 We can see the relationship in the following graph. The red line is the graphical manifistation of our model (pretty close to the original red line). gapminder %&gt;% ggplot(aes(x = gdpPercap, y = lifeExp)) + geom_point() + scale_x_log10(labels = scales::comma_format()) + geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;) + labs(x = &quot;Logged GDP/capita&quot;, y = &quot;Life Expectancy&quot;) 3.3 Advancing to Multiple Regression We see that there is a connection between the wealth of a country (logged GPDPercap) and lifeExp, but could this relationship be a function of the continent where someone resides? In bivariate regression you do not know about potentially confounding variables. Additionally, you may have more than just one hypothesis. Lots of research only specifies one relationship (a bi-variate relationship) when the real world is much more complicated. To get over this hurdle, we have introduce additional variables, thus changing from bivariate to multiple variable regression (sometimes referred to as multi-variate regression). The formula is very similar in R, but there are additional assumptions that need to be made (the next section). Here is the code for running a multiple regression: model2 &lt;- lm(lifeExp ~ log10(gdpPercap) + continent, data = gapminder) The line of code is almost exactly the same (I actually just copied and pasted this from above), with the only addition is the inclusion of + continent. That is it! Nothing too crazy to move beyond bivariate. But now you can make claims that you might have heard before, such as: “all else equal”, “holding everything constant”. But before we make claims, let’s check out the results. summary(model2) ## ## Call: ## lm(formula = lifeExp ~ log10(gdpPercap) + continent, data = gapminder) ## ## Residuals: ## Min 1Q Median 3Q Max ## -27.1163 -3.4739 0.4336 4.3519 18.5632 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.3170 1.3594 1.704 0.0885 . ## log10(gdpPercap) 14.7871 0.4225 35.003 &lt; 2e-16 *** ## continentAmericas 7.0147 0.5544 12.652 &lt; 2e-16 *** ## continentAsia 5.9117 0.4768 12.400 &lt; 2e-16 *** ## continentEurope 9.5771 0.6041 15.855 &lt; 2e-16 *** ## continentOceania 9.2135 1.5359 5.999 2.42e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.038 on 1698 degrees of freedom ## Multiple R-squared: 0.704, Adjusted R-squared: 0.7031 ## F-statistic: 807.6 on 5 and 1698 DF, p-value: &lt; 2.2e-16 We see from the results that the log10(gdpPercap) is still significant and positive, though the slope is a little smaller. We also see the output for four continents: Americas, Asia, Europe, Oceania. It appears that the gapminder dataset combines North and South America (something I wouldn’t recommend if this were my own data). Whenever we have a categorical variable like this, one category must be left out as the “holdout” or “baseline” category. In this instance, Africa is the holdout category (you can do some magic if you want a different baseline category, but it is generally not that important). All statements that are based on the findings for each continent has to be done in relationship to Africa. How do you interpret the model? Let’s start with GDP/capita. The interpretation is almost completely the same, but with a slight addition. Our model predicts that for a log10 unit increase in GDP/capita (going from 1-dollar per capita to 10-dollars per capita), we expect the average life expectancy for a country to increase by 14.7 years, all else equal. This means that while considering the continent of that country (or yet another way: regardless of the continent), there is still a positive relationship between GDP/capita and life expectancy. Interpreting the other variables is pretty simple. Given that we are comparing to Africa, we would say that countries in the Americas are predicted to have a 7 year higher average life expectancy, regardless of the GDP per capita (you could again say all else equal). Countries in Asia also have a higher life expectancy compared to Africa. Now I would only make these statements if there is a statistically significant relationship (generally determined based off of a p-value). If we have made the determination that a variable is not statistically significant, than we are unable to confidently say the direction of the effect. 3.4 Checking Assumptions Now we need to check for the different assumptions that go along with OLS regression. For those who want a quick introduction, this website seemed to have a good introduction. 3.4.1 Normality The first question is whether we have a normal distribution. The quantile-quantile (Q-Q) plot tests the assumption that our data really does come from a theoretically normal distribution (the code below requires the car package). You want the observations to fall along the blue line. In general, you will have some points off of the line. This is not a hard and fast rule. This assumption should be checked carefully for models that are small n (around 50). For this model, we are following fairly close to the line, and do not appear to be wildly violating the normality assumption. qqPlot(model2) ## [1] 853 854 a less informative, but easier to understand option is to do a histogram of residuals. Another more check on the residuals using a histogram. I am looking here to just see that our residuals are, overall, primarily around 0. We do see a little tail here to the left, but nothing that is too crazy. library(modelr) gapminder_results &lt;- gapminder %&gt;% add_residuals(model2) ggplot(gapminder_results, aes(resid)) + geom_histogram() + geom_vline(xintercept = 0, color = &quot;black&quot;, size = 2) Here we are seeing that our results appear to be somewhat normally distributed, but it does have a longer tail on the left. Another numeric technique to determine if our errors are normally distributed is the Jarque Bera Normality test (using the moments) package. From here we reject the null hypothesis that the errors are normally distributed. While not a true Gauss-Markov assumptions, we may want to consider using a generalized linear model which allows us to fit a different distribution. library(moments) ## Warning: package &#39;moments&#39; was built under R version 3.5.2 jarque.test(gapminder_results$resid) ## ## Jarque-Bera Normality Test ## ## data: gapminder_results$resid ## JB = 98.4, p-value &lt; 2.2e-16 ## alternative hypothesis: greater 3.4.2 Heteroskedasticity One of the assumptions of OLS is that you have constant variance in the residuals (homoskedasticity). Residuals are best described as the distance between your model and your actual data (they are also known as the error term or “left-overs”). You would expect that the distance between your data and the predicted outcome to be the same across all observations (across all continents and GDP/capita). Otherwise you will have a biased model. There are two different ways to look for Heteroskedasticity. First is the graphical look. Calling the plot() function with the model included will produce 4 graphs. You want to look primarily at the fist chart. If you do violate the assumption of homoskedasticity (hence, you have heteroskedasticity) then you would see equal distribution of point around 0 for across the x-axis. I would note that the other 3 charts are also checks on your model. The second chart is another Q-Q plot. Feel free to dive deeper into these charts, but I am going to move on. plot(model2) If you want a more formal test of heteroskedasticity, you use the car package’s ncvTest which is the Breush-Pagan test or Non-constant error variance test. For this one, you are looking at the p-value and making a normal hypothesis test with the null hypothesis being that there is homoskedasticity. ncvTest(model2) ## Non-constant Variance Score Test ## Variance formula: ~ fitted.values ## Chisquare = 49.29342, Df = 1, p = 2.204e-12 Here we definitely show significance and need to use some sort of fix for this problem. With heteroskedasticity, your results will be biased and inefficient. The most common way to fix this problem is to use Huber-White robust standard erros (also known as sandwich standard errors). This doesn’t actually fix the problem of inefficiency, but does help wih the bias. Using the lmtest and sandwich packages, we can obtain those robust standard errors. You will see that getting the coefficients are the same with both the lines is the same, but the standard errors, t-stats and p-values are different. If you wanted a different method, you could use Weighted Least Squares regression (WLS), but that is for another text. The WLS will be a better estimate than simply using robust standard errors, but that is only true if we have properly modeled the error variance. library(sandwich) library(lmtest) summary(model2) ## ## Call: ## lm(formula = lifeExp ~ log10(gdpPercap) + continent, data = gapminder) ## ## Residuals: ## Min 1Q Median 3Q Max ## -27.1163 -3.4739 0.4336 4.3519 18.5632 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.3170 1.3594 1.704 0.0885 . ## log10(gdpPercap) 14.7871 0.4225 35.003 &lt; 2e-16 *** ## continentAmericas 7.0147 0.5544 12.652 &lt; 2e-16 *** ## continentAsia 5.9117 0.4768 12.400 &lt; 2e-16 *** ## continentEurope 9.5771 0.6041 15.855 &lt; 2e-16 *** ## continentOceania 9.2135 1.5359 5.999 2.42e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.038 on 1698 degrees of freedom ## Multiple R-squared: 0.704, Adjusted R-squared: 0.7031 ## F-statistic: 807.6 on 5 and 1698 DF, p-value: &lt; 2.2e-16 coeftest(model2, vcov = vcovHC) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.31701 1.64432 1.4091 0.159 ## log10(gdpPercap) 14.78713 0.53360 27.7120 &lt;2e-16 *** ## continentAmericas 7.01468 0.65568 10.6983 &lt;2e-16 *** ## continentAsia 5.91171 0.58402 10.1225 &lt;2e-16 *** ## continentEurope 9.57713 0.65965 14.5185 &lt;2e-16 *** ## continentOceania 9.21348 0.81266 11.3374 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 You can use these results to present in your results. 3.4.3 Multicollinearity In multiple regression we assume that there is “no perfect multicollinearity”. If you are at this point, you don’t have this problem. R would not compute the model correctly if you had multicollienarity problems. To have a perfectly multicollinearity would happen if we included GDP per capita in dollars and Pesos. These variables would have a correlation of 1 because it is showing the exact same data. While we do not have to worry about perfect multicollinearity, we do want to see if there is high multicollinearity in our model. If we do, we basically have a redundant predictor and our model could be unstable when introduced to slight changes in the data. Using the car package, we want to check to see if we have a problem with multicollinearity by using the variance inflation factor or VIF. For each of our covariates we will get a VIF score. The minimum value is 1 and you should only start poking around if you have a score over 5. Though, as a rule of thumb, whenever your VIF exceeds 10, we can start to believe that our model is being shaped by the multicollinearity. To give you an idea, a VIF of 10 means that 90% of hte variance of one predictor can be explained by the other predictors. Unlike other test statistics though, there is no hard and fast number. You really have to take this in context of the findings. vif(model2) ## GVIF Df GVIF^(1/(2*Df)) ## log10(gdpPercap) 1.782678 1 1.33517 ## continent 1.782678 4 1.07494 In our case, the VIF scores for each variable fit in the normal range. No problems here, moving on. 3.4.4 Outliers and leverage If only a few observations are driving our results, then we may not be able to trust our model. There are three general types of problems (note, a lot of this discussion was adapted based off of Political Analysis Using R by James E. Monogon III): Outliers: observations with exceedingly large residuals Leverage points: takes a value of a predictor that is disproportionately distant from other values Influence points: outliers with a lot of leverage Influence points are particularly problemsome because they can mess up our model the most. By creating scatterplots, we can start to gain an idea of different problems. “If an observation stands out on the predictor’s scale then it has leverage. If it stands out on the residual scale then it is an outlier. If it stands out on both dimensions, then it is an influence point.” Using the influenceIndexPlot from the car package, we can see the different indicators that we are having an issue. For instance, a value of 1 for Cook’s distance would indicatre the presence of influential data points. Studentized residuals detect outliers, and hat values detect leverage points. We can see here that there are a few points that seem problematic, but it is up to the researcher to determine how to handle the data (you can delete them but you open yourself up to bias). influenceIndexPlot(model2, vars = c(&quot;Cook&quot;, &quot;Studentized&quot;, &quot;hat&quot;), id.n = 5) ## Warning in plot.window(...): &quot;id.n&quot; is not a graphical parameter ## Warning in plot.xy(xy, type, ...): &quot;id.n&quot; is not a graphical parameter ## Warning in axis(side = side, at = at, labels = labels, ...): &quot;id.n&quot; is not ## a graphical parameter ## Warning in axis(side = side, at = at, labels = labels, ...): &quot;id.n&quot; is not ## a graphical parameter ## Warning in box(...): &quot;id.n&quot; is not a graphical parameter ## Warning in title(...): &quot;id.n&quot; is not a graphical parameter ## Warning in plot.xy(xy.coords(x, y), type = type, ...): &quot;id.n&quot; is not a ## graphical parameter ## Warning in plot.xy(xy.coords(x, y), type = type, ...): &quot;id.n&quot; is not a ## graphical parameter ## Warning in plot.window(...): &quot;id.n&quot; is not a graphical parameter ## Warning in plot.xy(xy, type, ...): &quot;id.n&quot; is not a graphical parameter ## Warning in axis(side = side, at = at, labels = labels, ...): &quot;id.n&quot; is not ## a graphical parameter ## Warning in axis(side = side, at = at, labels = labels, ...): &quot;id.n&quot; is not ## a graphical parameter ## Warning in box(...): &quot;id.n&quot; is not a graphical parameter ## Warning in title(...): &quot;id.n&quot; is not a graphical parameter ## Warning in plot.xy(xy.coords(x, y), type = type, ...): &quot;id.n&quot; is not a ## graphical parameter ## Warning in plot.xy(xy.coords(x, y), type = type, ...): &quot;id.n&quot; is not a ## graphical parameter ## Warning in plot.window(...): &quot;id.n&quot; is not a graphical parameter ## Warning in plot.xy(xy, type, ...): &quot;id.n&quot; is not a graphical parameter ## Warning in axis(side = side, at = at, labels = labels, ...): &quot;id.n&quot; is not ## a graphical parameter ## Warning in axis(side = side, at = at, labels = labels, ...): &quot;id.n&quot; is not ## a graphical parameter ## Warning in box(...): &quot;id.n&quot; is not a graphical parameter ## Warning in title(...): &quot;id.n&quot; is not a graphical parameter ## Warning in plot.xy(xy.coords(x, y), type = type, ...): &quot;id.n&quot; is not a ## graphical parameter ## Warning in plot.xy(xy.coords(x, y), type = type, ...): &quot;id.n&quot; is not a ## graphical parameter 3.4.5 Autocorrelation NOTE: I am skipping this assumption because it is primarily concerning times series, but I am including a small discussion and code for discussing autocorrelation. Another assumption of OLS regression is that our errors are not correlated. This is grouped into the “independence” assumption of “iid”. Autocorrelation is less of an issue with cross-sectional studies and more with times series. For instance, if you are looking at the GDP of a country over time, you would expect that the best predictor of GDP or the U.S. is the GDP of the U.S. the previous year. This means that there is autocorrelation and we are violating the independence assumption.. To test for this assumption, we can use the Durbin Watson Test. You will get a D-W Statistic which will range from 0 to 4, with a score of 2 showing no autocorrelation. You will see a p-value that tests the null hypothesis that there is no correlation among the residual (meaning the residuals are independent). durbinWatsonTest(model2) ## lag Autocorrelation D-W Statistic p-value ## 1 0.7599568 0.4742002 0 ## Alternative hypothesis: rho != 0 dwtest(model2) ## ## Durbin-Watson test ## ## data: model2 ## DW = 0.4742, p-value &lt; 2.2e-16 ## alternative hypothesis: true autocorrelation is greater than 0 NEEDT TO DO SOMETHING ABOUT THE AUTOCORRELATION (PROBABLY AN AR(1)) At this point, I am satisfied that we are not seriously violating any assumption. Now I am at a point where I okay with moving onto predictions. While predictions are the fun part of a model, it is not advised until you do the hard work of checking assumptions. If you do this out of order, you risk giving predictions based off of a bad model. I particularly like the quote: “all models are wrong but some are useful.” - George E.P. Box 3.4.6 Residual Analysis Beyond the formal tests, it is also good to see how your model does with residuals against your different independent variables. The first code creates a new data frame (using the school_1 data) and then adds the residual for each observation. What follows is a series of plots to see how the residuals are distributed. Use this to see if your residuals are showing a pattern, or seem to be roughly random (or normally distributed). The first bit of code creates a new dataset, school_1_graduation using school_1 then uses the modelr package to add_residuals passing in our graduation_gpa_model. ggplot(gapminder_results, aes(resid)) + geom_density() ggplot(gapminder_results, aes(x = log10(gdpPercap), y = resid)) + geom_point() gapminder_results %&gt;% group_by(continent) %&gt;% mutate(squared_resid = resid^2) %&gt;% summarise(average_squared_resid = mean(squared_resid)) %&gt;% ggplot(aes(x = continent, y = average_squared_resid)) + geom_col() + labs(y = &quot;Average Squared Residual&quot;) ## Warning: package &#39;bindrcpp&#39; was built under R version 3.5.2 3.5 Displaying Results Now that you have done the analyses, you know if you need to make any changes before you start to display the results (it is not worth making predictions if we aren’t sure if the model is worth anything). 3.5.1 Tables One of the simplest methods to get a table is to use the stargazer command which comes from the stargazer package. The commands for this are pretty simple, you can simply call stargazer(model2, type = &quot;text&quot;) to get something that you can read in R. stargazer(model2, type = &quot;text&quot;) ## ## =============================================== ## Dependent variable: ## --------------------------- ## lifeExp ## ----------------------------------------------- ## log10(gdpPercap) 14.787*** ## (0.422) ## ## continentAmericas 7.015*** ## (0.554) ## ## continentAsia 5.912*** ## (0.477) ## ## continentEurope 9.577*** ## (0.604) ## ## continentOceania 9.213*** ## (1.536) ## ## Constant 2.317* ## (1.359) ## ## ----------------------------------------------- ## Observations 1,704 ## R2 0.704 ## Adjusted R2 0.703 ## Residual Std. Error 7.038 (df = 1698) ## F Statistic 807.634*** (df = 5; 1698) ## =============================================== ## Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 If you do not specify the type, the command will print out the \\(\\LaTeX\\) code to make the same table, which is very hard for a human to read the print out. There are a number of ways to customize the table to include even multiple models (great when you have slight variations). You can even show different output. stargazer(model1, model2, type = &quot;text&quot;, report = (&#39;vc*p&#39;)) ## ## ========================================================================= ## Dependent variable: ## ----------------------------------------------------- ## lifeExp ## (1) (2) ## ------------------------------------------------------------------------- ## log10(gdpPercap) 19.353*** 14.787*** ## p = 0.000 p = 0.000 ## ## continentAmericas 7.015*** ## p = 0.000 ## ## continentAsia 5.912*** ## p = 0.000 ## ## continentEurope 9.577*** ## p = 0.000 ## ## continentOceania 9.213*** ## p = 0.000 ## ## Constant -9.101*** 2.317* ## p = 0.000 p = 0.089 ## ## ------------------------------------------------------------------------- ## Observations 1,704 1,704 ## R2 0.652 0.704 ## Adjusted R2 0.652 0.703 ## Residual Std. Error 7.620 (df = 1702) 7.038 (df = 1698) ## F Statistic 3,192.273*** (df = 1; 1702) 807.634*** (df = 5; 1698) ## ========================================================================= ## Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 In the code above, I told stargazer to include both of the models we created earlier. In addition I used the report option to get different outputs: c: Coefficient V: Variable name *: significance stars p: p-values These are the four basic things I like to include, but there are many different options. As mentioned, this is not quite production ready. I would recommend actually having stargazer create a new word document usin the out option and then specifying a file path/name. Also, having the table type be html makes this a lot easier to manipulate. # stargazer(model1, model2, type = &quot;html&quot;, report = (&#39;cv*p&#39;), out = &quot;example table.doc&quot;) All of this being said, I would actually not use the stargazer table for model2 because we know we suffer from heteroskedasticity. Instead I would call the coeftest(model2, vcov = vcovHC) again and hand create a table from that. 3.5.2 Coefplots Coefficient Plots, also referred to as coefplots, are a great way to show visually what a standard table reports. Here we are using the broom package’s tidy() function to get a tidy data frame of our output. The tidy() function will return one row for every term (variable including the intercept) and a column for the term, estimate (coefficient), standard error, test statistic, and pvalue. tidy(model2) ## # A tibble: 6 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 2.32 1.36 1.70 8.85e- 2 ## 2 log10(gdpPercap) 14.8 0.422 35.0 1.50e-202 ## 3 continentAmericas 7.01 0.554 12.7 3.99e- 35 ## 4 continentAsia 5.91 0.477 12.4 7.30e- 34 ## 5 continentEurope 9.58 0.604 15.9 6.70e- 53 ## 6 continentOceania 9.21 1.54 6.00 2.42e- 9 The nicest thing about the tidy() function is that it allows you to use the tidyverse framework. In the code below I am creating a 95% confidence interval using the mutate command. First I am getting the low end of the confidence interval by subtracting the 1.96 * std.error from the coefficient estimate. The I repeat, but add to create the high end of estimate. Lastly, I am reordering the variable term knowing that later this will make the graph look nicer. Now I have everything I need to create a coefplot! tidy(model2) %&gt;% mutate(low = estimate - (1.96 * std.error), high = estimate + (1.96 * std.error), term = fct_reorder(term, estimate)) ## # A tibble: 6 x 7 ## term estimate std.error statistic p.value low high ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 2.32 1.36 1.70 8.85e- 2 -0.347 4.98 ## 2 log10(gdpPercap) 14.8 0.422 35.0 1.50e-202 14.0 15.6 ## 3 continentAmericas 7.01 0.554 12.7 3.99e- 35 5.93 8.10 ## 4 continentAsia 5.91 0.477 12.4 7.30e- 34 4.98 6.85 ## 5 continentEurope 9.58 0.604 15.9 6.70e- 53 8.39 10.8 ## 6 continentOceania 9.21 1.54 6.00 2.42e- 9 6.20 12.2 When graphing I have decided to make a geom_point with the x axis being the term, and the y to be location of the estimate. The next line allows us to get an idea of our 95% confidence interval using geom_pointrange(). This allows us to use our new variables low and high to create a line line. Then we add the geom_hline with a y-intercept of 0 to help the viewer determine if the variable is significant at a 95% confidence level. If the estimate or the confidence band (created by geom_pointrange) crosses the red line, then we cannot reject the null hypothesis. Lastly, I do a coord_flip() to make it easier to read the graph (will flip our axes so that the x now looks like the y and the y looks like the x) tidy(model2) %&gt;% mutate(low = estimate - (1.96 * std.error), high = estimate + (1.96 * std.error), term = fct_reorder(term, estimate)) %&gt;% ggplot(aes(x = term, y = estimate)) + geom_point() + geom_pointrange(aes(ymin = low, ymax = high)) + geom_hline(yintercept = 0, color = &quot;red&quot;, size = 1) + coord_flip() There are things I would change, such as the labels for the axes and the terms, but was made to show you how to do the basics. You should be able to google these small cosmetic changes. 3.5.3 Graphing predictions One of the most important aspects running tests is to show your results in a clean format. While tables and text are possible, graphs are almost always preferred. Here we will show three different graphs for predictions. The first graph shows the predictions based off of simulated data that varies gdpPercap. This is easily accomplished with the modelr package. In the code below, we first call the dataset. Then we call data_grid which gets requires one argument: the variable(s) you would like to have vary. If you specify a .model argument, it will fill all covariates at their typical value (mean or mode) of the data used in the model. If you run the first two lines, it will show two columns. The first is gdpPercap which has different values ranging from the minimum and maximum from the dataset used. The second column is our covariate (in this model we had two variables so there are only two columns here). The third line add_prediction(model2), tells R to take this new dataset and make predictions based off the results of model2. This command creates a third column called pred (predictions). Once you have this data, it becomes a simple ggplot() problem. Here I have used geom_line() to create a line graph. Because we used a logarithmic transformation, I have added the scale_x_log10(labels = comma_format()). This puts the x-axis on a logarithmic scale. The comma_format() is only used to make the graph easier to understand. gapminder %&gt;% data_grid(gdpPercap, .model = model2) %&gt;% add_predictions(model2) %&gt;% ggplot(aes(x = gdpPercap, y = pred)) + geom_line() + scale_x_log10(labels = comma_format()) The graph below shows similar results, but I added one layer of complexity. Here I told data_grid to vary not only gdpPercap but also continent. Now instead of 1704 rows we have 8520 (for those paying attention that is 1704 multiplied by the number of continents). The only other change is that I added the color = contintent argument to create a line for each continent. You will see that each line is parallel to each other. The difference between each line is the coefficient for each continent. gapminder %&gt;% data_grid(gdpPercap, continent, .model = model2) %&gt;% add_predictions(model2) %&gt;% ggplot(aes(x = gdpPercap, y = pred, color = continent)) + geom_line() + scale_x_log10(labels = scales::comma_format()) The last graph to show is a bar chart (though I used geom_col()) of the different predictions for each continent. This uses almost the same exact code to create the data_grid. To make the graph look a little nicer, I decided to reorder the continents by prediction continent = fct_reorder(continent, pred). This isn’t necessary, but a nice addition. gapminder %&gt;% data_grid(continent, .model = model2) %&gt;% add_predictions(model2) %&gt;% mutate(continent = fct_reorder(continent, pred)) %&gt;% ggplot(aes(x = continent, y = pred)) + geom_col() ?add_predictions ## starting httpd help server ... done "],
["logit-models.html", "Chapter 4 Logit Models 4.1 Introducing Logit Models 4.2 Example of Logit Model 4.3 Adding Predictions 4.4 How did our model do? 4.5 Final Words", " Chapter 4 Logit Models 4.1 Introducing Logit Models There are a number of different models that can be run with a binary regression model (BRM) including: Linear Probability Model, Logit Model, Probit Mode, and the Log-Log model. Most commonly we choose between the Logit and Probit model. The logit model in the past has been the default because the error distribution assumptions decreased the computing cost for the probability distribution function. The decision between the logit and probit are essentially arbitrary because the major difference between them (the distribution of the error term) is something we cannot tested. When specifying a BRM, we make three main assumptions: The threshold for 0 is \\(\\tau = 0\\) The Conditional mean of \\(\\epsilon\\) is 0: \\(E(\\epsilon | X) = 0\\) The conditional variance of \\(\\epsilon\\) is constant: \\(Var(\\epsilon|X = 1)\\) for probit models and \\(Var(\\epsilon|X = \\pi/3)\\) for logit models. We also assume that there is an unobserved latent variable that we cannot fully observe, but only observe the cut point (pass/fail). We assume that our observed x’s are linearly related to the latent variable \\(y^*\\). 4.2 Example of Logit Model To show an example of a logit model at work, we will examine the titanic dataset. Specifically we will use the titanic package and load the titanic_train dataset. The Dependent Variable is Survived which indicates whether the person survived. Our regressors in this analysis are: Passenger Class (Pclass), Sex (Sex), and Age (Age). library(titanic) ## Warning: package &#39;titanic&#39; was built under R version 3.5.3 library(tidyverse) library(modelr) titanic &lt;- titanic_train Our model in this case is: \\(Pr(Surv = 1) = F(\\beta_0 + \\beta_1Class + \\beta_2Sex + \\beta_3AGe + \\beta_4Cabin)\\) Now lets specify the model in R: model &lt;- glm(Survived ~ Pclass + Sex + Age, family = &quot;binomial&quot;, data = titanic) The code to run the model is very similar to that of an OLS regression. The left hand side of the assignment operator, &lt;-, is the name of the object you are creating. On the right is the actual model you are running. First is the glm which standards for Generalized Linear Models. Just like with OLS regression, the dependent variable is on the left of the tilde ~. The right side includes the independent variables. In our case the dependent variable is Survived and the independent variables include: Pclass, Sex, and age. The family option tells the glm() function the type of dependent variable you have. In this case we have a binomial dependent variable. Unless you specify otherwise, the default link function is the a logit model. See this quick page describing the basics of the glm() function. Lastly, we always have to call our dataset. Now let’s take a look at the model. summary(model) ## ## Call: ## glm(formula = Survived ~ Pclass + Sex + Age, family = &quot;binomial&quot;, ## data = titanic) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.7270 -0.6799 -0.3947 0.6483 2.4668 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 5.056006 0.502128 10.069 &lt; 2e-16 *** ## Pclass -1.288545 0.139259 -9.253 &lt; 2e-16 *** ## Sexmale -2.522131 0.207283 -12.168 &lt; 2e-16 *** ## Age -0.036929 0.007628 -4.841 1.29e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 964.52 on 713 degrees of freedom ## Residual deviance: 647.29 on 710 degrees of freedom ## (177 observations deleted due to missingness) ## AIC: 655.29 ## ## Number of Fisher Scoring iterations: 5 When it comes to variables, we are looking for directionality and significance (we are given log odds for the coefficient estimate). Remember that the dependent variable is whether the passenger survived. Each of our variables are negative meaning that as they increase, the probability of surviving the titanic crash is lowered. Specifically, as you go up in passenger class and age you are less likely to survive. Men (compared to women) are less likely to survive. All three independent variables are significant at virtually all accepted levels. Lastly, we notice that our AIC is listed at 655.29. This is not informative by itself, but can help when comparing this to other models. In its raw form, coefficient estimates cannot tell us magnitude. Additional steps are necessary to get predictions. If we did, however, want to get something of substance to report in the talbe, you can take the exponent of the estimate. This will give you the odds ratio. Monogan states that: “the odds of an event is the ratio fo the probability the event occurs to the probabiliity it does not occur \\(\\dfrac{p}{1- p}\\). The odds ratio tells us the multiplicatvie factor by which the odds will change for a unit increase in the predictor.” We can compute it as follows: exp(model$coefficients[-1]) ## Pclass Sexmale Age ## 0.27567157 0.08028834 0.96374454 In the code above, I excluded the intercept with [-1]. The 0.963 indicates that as you go up in age by 1 year, theodds that you will survive decrease by 0.96, all else equal. If you prefer percentages: 100 * (exp(model$coefficients[-1])-1) ## Pclass Sexmale Age ## -72.432843 -91.971166 -3.625546 In this case as you increase in age, your odds of surviving decrease by 3.6%. 4.3 Adding Predictions Given that the direct output of a logit model is not particularly helpful. It is nice to create graphs of predicted probabilities. These graphs, generally, create a simulated dataset that is similar to the dataset used to create the model, but will vary one variable while holding the others constant. The cleanest way that I know of how to make a new dataset is to use the data_grid. The code below calls our original dataset titanic, then calls the data_grid function. Within the data_grid function you call the variable you wish to vary, in our case I picked Age. the .model = model tells data_grid that we want to use all of the predictors in the model we ran, filling them at their typical values (mean or mode). For some reason, the function seems to want to always add in a last NA row at the end so I am filtering it out. Continuing in the tidy format, we call the add_predictions(model, type = &quot;respone&quot;) which comes from the modelr package. Here we are taking this new dataset we have made with data_grid and adding predictiosn based off of the results of our model. The key difference in this line of code compared to OLS is the additional argument of type = &quot;response&quot; which tells R that we want predicted probabilities. After creating the predictions, it now becomes a simple graphing problem of figuring out what kind of variable you have for the x-axis and determining the appropriate graph. In this case, the most appropiate would be either a line graph or scatterplot. titanic %&gt;% data_grid(Age, .model = model) %&gt;% filter(!is.na(Age)) %&gt;% add_predictions(model, type = &quot;response&quot;) %&gt;% ggplot(aes(x = Age, y = pred)) + geom_point() Now let’s repeat the process for gender: titanic %&gt;% data_grid(Sex, .model = model) %&gt;% add_predictions(model, type = &quot;response&quot;) %&gt;% ggplot(aes(x = Sex, y = pred)) + geom_col() Basically, don’t be a guy if you want to survive. &lt;&gt; 4.4 How did our model do? One fun way to see how the model performs is to look at a confusion matrix. To create a confusion matrix, have hte model spit out probabilities on the actual data. Then create a new variable that is equal to 1 if the probability is .50 or higher and 0 otherwise. Then you simply have a table that compares the predicted outcome (1,0) to the actual (1,0). The ones that are in the top left and bottom right are the ones that the model correctly predicted, with the other two spots as the mis-classifications. confusion &lt;- titanic prediction &lt;- predict(model, confusion, type = &quot;response&quot;) predicted_classes &lt;- ifelse(prediction &gt; 0.5, 1, 0) predicted_classes &lt;- as.factor(predicted_classes) table(titanic$Survived, predicted_classes) ## predicted_classes ## 0 1 ## 0 356 68 ## 1 83 207 In this case our model correct guessed 356 people not surviving and 207 people as surviving. It incorrectly guessed at 83 would die that actually survived and 68 as surviving who actually died. If we take the correct over total we get the following: (356 + 207)/(356 + 207 + 83 + 68) ## [1] 0.7885154 So we can correctly predict 78.8%. This is a good improvement than if we just guessed everyone drowned (the model category). table(titanic$Survived) ## ## 0 1 ## 549 342 549/(549 + 342) ## [1] 0.6161616 4.5 Final Words This introduction ignored the assumptions that underly logit models. We also ignored probit models. For a better understanding of how these models work, I recommend “Regression Models for Categorical and Limited Dpeendent Variables” by J. Scott Long "],
["ordered-logit-models.html", "Chapter 5 Ordered Logit Models 5.1 Data and Packages 5.2 Running the Model 5.3 Gettin predictions: 5.4 Showing predictions 5.5 Graphing in a Tidy Way", " Chapter 5 Ordered Logit Models 5.1 Data and Packages library(tidyverse) library(modelr) library(haven) ## Warning: package &#39;haven&#39; was built under R version 3.5.2 library(broom) satisfaction &lt;- read_dta(&quot;http://j.mp/SINGHejpr&quot;) In this dataset we want to model each respondent’s level of satisfaction with democracy. This variable can be a 1, 2, 3, 4. We can say they are ordered, but we don’t know the distance. 4 is Very satisfied. Because they are ordered, we should run an ordered logit/probit. 5.2 Running the Model We will likely want to use the polr (proportional odds logistic regression) command from the MASS package. library(MASS) ## ## Attaching package: &#39;MASS&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## select library(effects) ## Warning: package &#39;effects&#39; was built under R version 3.5.2 ## lattice theme set by effectsTheme() ## See ?effectsTheme for details. satisfaction$satisfaction &lt;- ordered(as.factor(satisfaction$satisfaction)) ideology_satis &lt;- polr(satisfaction ~ voted_ideo*winner + abstained + educ + efficacy + majoritarian_prez + freedom + gdppercapPPP + gdpgrowth + CPI + prez, method = &quot;logistic&quot;, data = satisfaction) summary(ideology_satis) ## ## Re-fitting to get Hessian ## Call: ## polr(formula = satisfaction ~ voted_ideo * winner + abstained + ## educ + efficacy + majoritarian_prez + freedom + gdppercapPPP + ## gdpgrowth + CPI + prez, data = satisfaction, method = &quot;logistic&quot;) ## ## Coefficients: ## Value Std. Error t value ## voted_ideo -0.02170 0.023596 -0.9198 ## winner 0.21813 0.020638 10.5694 ## abstained -0.25425 0.020868 -12.1838 ## educ 0.08238 0.020180 4.0824 ## efficacy 0.16246 0.006211 26.1569 ## majoritarian_prez 0.05705 0.018049 3.1609 ## freedom 0.04770 0.014087 3.3863 ## gdppercapPPP 0.01975 0.001385 14.2578 ## gdpgrowth 0.06653 0.003188 20.8673 ## CPI -0.23153 0.005810 -39.8537 ## prez -0.11503 0.026185 -4.3930 ## voted_ideo:winner 0.19004 0.037294 5.0957 ## ## Intercepts: ## Value Std. Error t value ## 1|2 -2.0501 0.0584 -35.1284 ## 2|3 -0.0588 0.0575 -1.0228 ## 3|4 2.7315 0.0586 46.6423 ## ## Residual Deviance: 146397.33 ## AIC: 146427.33 Note that the polr command requires that our outcome variable is ordered numerically. Also notice the interaction term. While the output shows t-values, they are actually z values since maximum likelihood methods typically call for z ratios. Lastly, note the different intercept cutpoints. These are generally not of interest to us, but are useful when it comes to making predictions. If we wanted to get the p-value of a specific variable (voted_ideo:winner for instance) then we could run the following code with the z ratio. 1 - pnorm(5.0957) ## [1] 1.737275e-07 We can conclude with 99.9% confidence that the coefficient on the interaction term is greater than zero. A nice teature of using the logit link function is that the results can be inerpreted in terms of odd ratios, though they have to be computed differently for ordinal models compared to logit models. FOr logit models, we must exponentiate the negative value of a coefficient and interpret the odds of being in lower groups relative to higher groups. For example: the odds ratio for our coefficients from the model above can be produced by the following code: 100*(exp(-ideology_satis$coefficients)-1) ## voted_ideo winner abstained educ ## 2.194186 -19.597657 28.949139 -7.908003 ## efficacy majoritarian_prez freedom gdppercapPPP ## -14.994659 -5.545347 -4.658313 -1.955471 ## gdpgrowth CPI prez voted_ideo:winner ## -6.436961 26.053177 12.190773 -17.307376 If, for instance, we wanted to interpret the influence of efficiency, then we could say that for a one point incrase on a five point efficacy scale, the odds a respondent will report that they are “not at all satisfied” with democracy relative to any of the three categories decrease by 15%, all else equal. Also, the odds that a respondent will report “not at all satisfied” or “not very satisfied” relative to the two higher categories also decrease by 15%, all else equal. In general then, we can interpret the oddds ratio for an ordered logit as shaping the odds of all optiosn below a threshold relative to all options above a threshold. 5.3 Gettin predictions: Here I am doing the discrete class predicted, not the predicted probabilities. If you want to see predicted probabilities, see the section of Graphing the Tidy Way. Somewhat uninteresting is that all of them are expected to be in the 3rd category. new_data &lt;- satisfaction %&gt;% data_grid(efficacy, .model = ideology_satis) new_data &lt;- new_data %&gt;% mutate(pred_class = predict(ideology_satis, new_data, type = &quot;class&quot;)) new_data ## # A tibble: 5 x 12 ## efficacy voted_ideo winner abstained educ majoritarian_pr~ freedom ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0 0 0 0 0 -1 ## 2 2 0 0 0 0 0 -1 ## 3 3 0 0 0 0 0 -1 ## 4 4 0 0 0 0 0 -1 ## 5 5 0 0 0 0 0 -1 ## # ... with 5 more variables: gdppercapPPP &lt;dbl&gt;, gdpgrowth &lt;dbl&gt;, ## # CPI &lt;dbl&gt;, prez &lt;dbl&gt;, pred_class &lt;fct&gt; 5.4 Showing predictions Effect(focal.predictors = c(&quot;voted_ideo&quot;, &quot;winner&quot;), ideology_satis) ## ## Re-fitting to get Hessian ## ## voted_ideo*winner effect (probability) for 1 ## winner ## voted_ideo 0 0.2 0.5 0.8 1 ## 0 0.07738718 0.07432926 0.06995037 0.06581111 0.06317929 ## 0.2 0.07769769 0.07410523 0.06900243 0.06422663 0.06121568 ## 0.5 0.07816558 0.07377033 0.06760280 0.06191642 0.05837700 ## 0.8 0.07863605 0.07343684 0.06622953 0.05968401 0.05566216 ## 1 0.07895114 0.07321528 0.06532847 0.05823785 0.05391873 ## ## voted_ideo*winner effect (probability) for 2 ## winner ## voted_ideo 0 0.2 0.5 0.8 1 ## 0 0.3031896 0.2960183 0.2852714 0.2745687 0.2674749 ## 0.2 0.3039029 0.2954823 0.2828681 0.2703255 0.2620302 ## 0.5 0.3049728 0.2946783 0.2792682 0.2639884 0.2539215 ## 0.8 0.3060424 0.2938744 0.2756754 0.2576905 0.2458946 ## 1 0.3067553 0.2933386 0.2732849 0.2535167 0.2405948 ## ## voted_ideo*winner effect (probability) for 3 ## winner ## voted_ideo 0 0.2 0.5 0.8 1 ## 0 0.5285674 0.5351282 0.5445027 0.5532844 0.5587926 ## 0.2 0.5279015 0.5356088 0.5465232 0.5566096 0.5628467 ## 0.5 0.5268982 0.5363271 0.5494974 0.5614062 0.5685967 ## 0.8 0.5258898 0.5370422 0.5524023 0.5659676 0.5739373 ## 1 0.5252147 0.5375173 0.5542999 0.5688746 0.5772640 ## ## voted_ideo*winner effect (probability) for 4 ## winner ## voted_ideo 0 0.2 0.5 0.8 1 ## 0 0.09085585 0.09452425 0.1002756 0.1063358 0.1105532 ## 0.2 0.09049792 0.09480369 0.1016063 0.1088383 0.1139074 ## 0.5 0.08996341 0.09522425 0.1036317 0.1126889 0.1191048 ## 0.8 0.08943174 0.09564648 0.1056927 0.1166580 0.1245060 ## 1 0.08907887 0.09592889 0.1070867 0.1193709 0.1282225 plot(Effect(focal.predictors = c(&quot;voted_ideo&quot;, &quot;winner&quot;), ideology_satis)) ## ## Re-fitting to get Hessian plot(Effect(focal.predictors = &quot;efficacy&quot;, ideology_satis)) ## ## Re-fitting to get Hessian 5.5 Graphing in a Tidy Way I am not sure if I have greally graphed this correctly, but this is what I could come up with. pred &lt;- predict(ideology_satis, type = &quot;probs&quot;) prediction &lt;- cbind(satisfaction, pred) prediction %&gt;% rename(&quot;one&quot; = 22, &quot;two&quot; = 23, &quot;three&quot; = 24, &quot;four&quot; = 25) %&gt;% dplyr::select(efficacy, one, two, three, four) %&gt;% gather(outcome, prediction, -efficacy) %&gt;% mutate(outcome = factor(outcome, levels = c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;, &quot;four&quot;))) %&gt;% ggplot(aes(x = prediction, fill = as.factor(efficacy))) + geom_histogram() + facet_wrap(efficacy~outcome, ncol = 4) + labs(fill = &quot;Efficacy Levels&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. "],
["multi-level-models.html", "Chapter 6 Multi Level Models 6.1 Introduction 6.2 GLMs 6.3 Repeated Measures", " Chapter 6 Multi Level Models 6.1 Introduction library(lme4) ## Warning: package &#39;lme4&#39; was built under R version 3.5.2 ## Loading required package: Matrix ## ## Attaching package: &#39;Matrix&#39; ## The following object is masked from &#39;package:tidyr&#39;: ## ## expand library(foreign) library(broom) library(tidyverse) Fixed-effect parameters only use data for a specific group. In contrast, random-effect parameters assume data share a common distribution. FOr situations with small amounts of data or ourliers, random effect models can produce different estimates. evolution&lt;-read.dta(&quot;http://j.mp/BPchap7&quot;) evolution$female[evolution$female==9]&lt;-NA evolution&lt;-subset(evolution,!is.na(female)) 6.1.1 full model hours_ml&lt;-lmer(hrs_allev ~ phase1 + senior_c + ph_senior + notest_p + ph_notest_p + female + biocred3 + degr3 + evol_course + certified + idsci_trans + confident + (1|st_fip), data = evolution) 6.1.2 building the model Here we are building a random effects with no fixed effects: initial &lt;- lmer(hrs_allev ~ (1 | st_fip), data = evolution) summary(initial) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: hrs_allev ~ (1 | st_fip) ## Data: evolution ## ## REML criterion at convergence: 6049 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.6954 -0.7003 -0.2460 0.6036 3.5975 ## ## Random effects: ## Groups Name Variance Std.Dev. ## st_fip (Intercept) 5.046 2.246 ## Residual 74.952 8.657 ## Number of obs: 841, groups: st_fip, 49 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 14.222 0.477 29.82 plot(initial) Now let’s build one with a fixed-effect slope parameter: fixed &lt;- lmer(hrs_allev ~ phase1 + (1 | st_fip), data = evolution) summary(fixed) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: hrs_allev ~ phase1 + (1 | st_fip) ## Data: evolution ## ## REML criterion at convergence: 6044.3 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.7107 -0.6892 -0.2430 0.6116 3.6573 ## ## Random effects: ## Groups Name Variance Std.Dev. ## st_fip (Intercept) 4.65 2.156 ## Residual 74.79 8.648 ## Number of obs: 841, groups: st_fip, 49 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 14.2695 0.4672 30.541 ## phase1 0.9433 0.4425 2.132 ## ## Correlation of Fixed Effects: ## (Intr) ## phase1 0.058 Now to add a random slope: random_slopes &lt;- lmer(hrs_allev ~ phase1 + (notest_p | st_fip), data = evolution) broom::tidy(random_slopes) ## Warning in bind_rows_(x, .id): binding factor and character vector, ## coercing into character vector ## Warning in bind_rows_(x, .id): binding character and factor vector, ## coercing into character vector ## # A tibble: 6 x 5 ## term estimate std.error statistic group ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 (Intercept) 14.3 0.458 31.2 fixed ## 2 phase1 0.829 0.424 1.96 fixed ## 3 sd_(Intercept).st_fip 2.34 NA NA st_fip ## 4 sd_notest_p.st_fip 0.924 NA NA st_fip ## 5 cor_(Intercept).notest_p.st_fip -1.000 NA NA st_fip ## 6 sd_Observation.Residual 8.65 NA NA Residual If we wanted to assume uncorrelated random-effect slopes (it is actually easier to calculate) all we have to do is turn | into || (though you likely want a reason to do this, I am just doing here for instruction) uncor &lt;- lmer(hrs_allev ~ phase1 + (notest_p || st_fip), data = evolution) ## singular fit summary(uncor) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: hrs_allev ~ phase1 + ((1 | st_fip) + (0 + notest_p | st_fip)) ## Data: evolution ## ## REML criterion at convergence: 6044.3 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.7107 -0.6892 -0.2430 0.6116 3.6573 ## ## Random effects: ## Groups Name Variance Std.Dev. ## st_fip (Intercept) 4.65 2.156 ## st_fip.1 notest_p 0.00 0.000 ## Residual 74.79 8.648 ## Number of obs: 841, groups: st_fip, 49 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 14.2695 0.4672 30.541 ## phase1 0.9433 0.4425 2.132 ## ## Correlation of Fixed Effects: ## (Intr) ## phase1 0.058 ## convergence code: 0 ## singular fit You can also have a variable as a fixed effect while correcting for random slopes: # fixed_random &lt;- lmer(hrs_allev ~ phase1 + (phase1 || st_fip), data = evolution) # # summary(fixed_random) 6.1.3 Understanding and reporting the outputs Point estimates # fixef(fixed_random) # ranef(fixed_random) We can also get confidence interals for the fixed effects using confint() # confint(fixed_random) Using the broom.mixed package, you can use the tidy() function to extract model results, though this isn’t as tidy as most models are: library(broom) # tidy(fixed_random, conf.int = TRUE) 6.1.4 Communicating results This is not very easy to extract lmer results (see link). # # Extract out the parameter estimates and confidence intervals and manipulate the data # dataPlot &lt;- data.frame(cbind( fixef(fixed_random), confint(fixed_random)[ 4:5, ])) # getting the rows for confint # rownames(dataPlot)[1] &lt;- &quot;Intercept&quot; # colnames(dataPlot) &lt;- c(&quot;est&quot;, &quot;L95&quot;, &quot;U95&quot;) # dataPlot$parameter &lt;- rownames(dataPlot) # # # Print the new dataframe # print(dataPlot) # # # Plot the results using ggplot2 # ggplot(dataPlot, aes(x = parameter, y = est, ymin = L95, ymax = U95)) + # geom_hline( yintercept = 0, color = &#39;red&#39; ) + # geom_linerange() + # geom_point() + # coord_flip() + # theme_minimal() It is important that in many instances, rescaling might have to occur to make the model numerically viable (for instance, changing the year in our study to be 0 instead of the actual year). 6.1.5 Model comparison with Anova What happens if we include more variables or model it differently? Well we can use anove to determine which model is better: # anova(random_slopes, fixed_random) In this instance the fixed_random does a better job, but is not significantly better. 6.2 GLMs 6.2.1 Binomial Data code: glmer(y ~ x + (1 | group), family = ‘error term’) Here we are using the Bang dataset which is a survey of contraception use for women in Bangladesh. The variables we are using for the model is whether they used the contraception (user), then the number of living children they had (living.children), the age centered around the mean (age_mean), and whether they lived in an urban or rural area (urban). The random intercept in this case is the district where they reside. library(epiDisplay) ## Warning: package &#39;epiDisplay&#39; was built under R version 3.5.3 ## Loading required package: survival ## Loading required package: nnet ## ## Attaching package: &#39;epiDisplay&#39; ## The following object is masked from &#39;package:lmtest&#39;: ## ## lrtest ## The following object is masked from &#39;package:scales&#39;: ## ## alpha ## The following object is masked from &#39;package:ggplot2&#39;: ## ## alpha data(Bang) cont_model &lt;- glmer(user ~ living.children + age_mean + urban + (1 | district), family = &quot;binomial&quot;, data = Bang) Now let’s look at the results summary(cont_model) ## Generalized linear mixed model fit by maximum likelihood (Laplace ## Approximation) [glmerMod] ## Family: binomial ( logit ) ## Formula: user ~ living.children + age_mean + urban + (1 | district) ## Data: Bang ## ## AIC BIC logLik deviance df.resid ## 2453.9 2481.7 -1221.9 2443.9 1929 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.9844 -0.7621 -0.5338 1.0010 2.4039 ## ## Random effects: ## Groups Name Variance Std.Dev. ## district (Intercept) 0.2156 0.4643 ## Number of obs: 1934, groups: district, 60 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.855018 0.183266 -10.122 &lt; 2e-16 *** ## living.children 0.421155 0.057540 7.319 2.49e-13 *** ## age_mean -0.030872 0.007828 -3.944 8.02e-05 *** ## urban 0.722556 0.118302 6.108 1.01e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) lvng.c age_mn ## lvng.chldrn -0.876 ## age_mean 0.606 -0.704 ## urban -0.254 0.097 -0.051 Everything is significant, and pretty much what we would expect. The more kids, the more likely you are to use it, The older you are, the less likely you are. And women in urban areas use them more. The coefficients however, are difficult to read becasue they are in log-dds. We can change this to odds ratios by taking the exponent. Here is a short explanation of odds-ratios: If an odds-ratio is 1.0, then both events have an equal chance of occurring. For example, if the odds-ratio for a urban was 1.0, then living in an urban/rural area would have no influence on contraception use. If an odds-ratio is less than 1, then living in an urban area would decrease the chance of using contraception. For example, an odds-ratio of 0.5 would mean a those living in urban areas have odds of 1:2 or 1 woman using contraception for every 2 women in rural areas. If an odds-ratio is greater than 1, then living in an urban area would increase the chance of contraception use. For example, an odds-ratio of 3.0 would mean living in an urban area has odds of 3:1 or 3 women using contraception in an urban area compared to those in rural areas. # calculating odds ratios and the confidence intervals for them exp(fixef(cont_model)) ## (Intercept) living.children age_mean urban ## 0.1564501 1.5237206 0.9696001 2.0596918 exp(confint(cont_model)) ## Computing profile confidence intervals ... ## 2.5 % 97.5 % ## .sig01 1.3870276 1.8949934 ## (Intercept) 0.1085024 0.2229839 ## living.children 1.3624164 1.7076706 ## age_mean 0.9546908 0.9844856 ## urban 1.6334721 2.5982986 6.2.2 Count models pretty simple change in formula: glmer(y ~ x + (1 | group), family = 'poisson') Here we are using data about chlamydia given by the state of illinois. We basically want to see if things are getting better over time, and whether age groups have this problem at different rates. chlamydia &lt;- read_csv(&quot;https://assets.datacamp.com/production/repositories/1803/datasets/612bd6490500636efa74132bfbc37817f250cb5a/ILdata.csv&quot;) ## Parsed with column specification: ## cols( ## age = col_character(), ## year = col_double(), ## county = col_character(), ## count = col_double() ## ) count_chlamydia &lt;- glmer(count ~ age + year + (year | county), family = &quot;poisson&quot;, data = chlamydia) ## Warning in checkConv(attr(opt, &quot;derivs&quot;), opt$par, ctrl = ## control$checkConv, : Model failed to converge with max|grad| = 0.00144074 ## (tol = 0.001, component 1) summary(count_chlamydia) ## Generalized linear mixed model fit by maximum likelihood (Laplace ## Approximation) [glmerMod] ## Family: poisson ( log ) ## Formula: count ~ age + year + (year | county) ## Data: chlamydia ## ## AIC BIC logLik deviance df.resid ## 3215.6 3259.6 -1599.8 3199.6 1800 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.4511 -0.0151 -0.0056 -0.0022 4.0053 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## county (Intercept) 129.9459 11.3994 ## year 0.0648 0.2546 -1.00 ## Number of obs: 1808, groups: county, 47 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -10.76258 2.13022 -5.052 4.36e-07 *** ## age20_24 -0.04152 0.03690 -1.125 0.261 ## age25_29 -1.16262 0.05290 -21.976 &lt; 2e-16 *** ## age30_34 -2.28278 0.08487 -26.898 &lt; 2e-16 *** ## year 0.32708 0.25422 1.287 0.198 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) a20_24 a25_29 a30_34 ## age20_24 -0.008 ## age25_29 -0.006 0.341 ## age30_34 -0.004 0.213 0.148 ## year -0.764 0.000 0.000 0.000 ## convergence code: 0 ## Model failed to converge with max|grad| = 0.00144074 (tol = 0.001, component 1) So there is a difference by age group for 2 categories. Now let’s show them in a good format: fixef(count_chlamydia) ## (Intercept) age20_24 age25_29 age30_34 year ## -10.76258497 -0.04151848 -1.16262225 -2.28277972 0.32708039 ranef(count_chlamydia) ## $county ## (Intercept) year ## ALEXANDER -0.2847724 0.006331741 ## BROWN -0.2847724 0.006331741 ## CALHOUN -0.2847724 0.006331741 ## CARROLL 12.2418514 -0.260423999 ## CASS -0.2847724 0.006331741 ## CLARK 12.2137668 -0.268553354 ## CLAY -0.2847724 0.006331741 ## CRAWFORD 12.5037407 -0.265752695 ## CUMBERLAND -0.2847724 0.006331741 ## DE WITT 12.7456078 -0.277675211 ## DOUGLAS 13.0751590 -0.306329903 ## EDGAR 12.3642794 -0.283606045 ## EDWARDS -0.2847724 0.006331741 ## FAYETTE 12.8094530 -0.273060474 ## FORD -0.2847724 0.006331741 ## GALLATIN -0.2847724 0.006331741 ## GREENE -0.2847724 0.006331741 ## HAMILTON -0.2847724 0.006331741 ## HANCOCK 12.8581265 -0.305650287 ## HARDIN -0.2847724 0.006331741 ## HENDERSON -0.2847724 0.006331741 ## IROQUOIS 13.1616741 -0.311372907 ## JASPER -0.2847724 0.006331741 ## JERSEY 12.9202747 -0.272284048 ## JO DAVIESS 12.7409389 -0.289747791 ## JOHNSON -0.2847724 0.006331741 ## LAWRENCE 12.3713561 -0.268571236 ## MARSHALL -0.2847724 0.006331741 ## MASON -0.2847724 0.006331741 ## MENARD -0.2180916 0.004849989 ## MERCER 12.7534193 -0.271678572 ## MOULTRIE -0.2180916 0.004849989 ## PIATT 12.5653132 -0.296687752 ## PIKE 12.5310614 -0.259211299 ## POPE -0.2180916 0.004849989 ## PULASKI -0.2180916 0.004849989 ## PUTNAM -0.2180916 0.004849989 ## RICHLAND 12.0350865 -0.273928951 ## SCHUYLER -0.2180916 0.004849989 ## SCOTT -0.2180916 0.004849989 ## SHELBY 12.5183293 -0.283472292 ## STARK -0.2180916 0.004849989 ## UNION 13.1465272 -0.308673332 ## WABASH -0.2180916 0.004849989 ## WASHINGTON -0.2180916 0.004849989 ## WAYNE 12.1148896 -0.253234752 ## WHITE -0.2180916 0.004849989 ## ## with conditional variances for &quot;county&quot; We can plot this in a similar fashion using ggplot, though it won’t be exactly the same as the glmer() outputs: (UGLY) # fits 4 graphs with I believe 47 lines for the actual data and 47 predicted ggplot(chlamydia, aes(x = year, y = count, group = county)) + geom_line() + facet_grid(age ~ .) + stat_smooth(method = &quot;glm&quot;, method.args = list( family = &quot;poisson&quot;), se = FALSE, alpha = 0.5) + theme_minimal() I think this still works, but gives a few number of counties: chlamydia %&gt;% filter(county == sample(county, 10)) %&gt;% ggplot(aes(x = year, y = count, group = county)) + geom_line() + facet_grid(age ~ .) + stat_smooth(method = &quot;glm&quot;, method.args = list( family = &quot;poisson&quot;), se = FALSE, alpha = 0.5) + theme_minimal() ## Warning in county == sample(county, 10): longer object length is not a ## multiple of shorter object length 6.3 Repeated Measures They are a special case of mixed-effects models. Following individuals through time. We may do a paired, t-test. You can also do a repeated measures ANOVA from more than just 2 tests (it could be test scores for every year of score). There is no default measure for running a repeated measure ANOVA by doing: library(lmertest) anova(lmer(y ~ time + (1|individual))) This can also do this for lmer nad glmer. Need to add a time variable and a group variable. 6.3.1 Paired T-test We are going to show this with randomly simulated data of 10 observations. # Set the seed to be 345659 set.seed(345659) # simulate before with mean of 0 and sd of 0.5 before &lt;- rnorm(10, mean = 0, sd = 0.5) # simulate after with mean effect of 4.5 and standard devation of 5 after &lt;- before + rnorm(10, mean = 4.5, sd = 5) # Run a standard, non-paired t-test t.test(x = before, y =after, paired = FALSE) ## ## Welch Two Sample t-test ## ## data: before and after ## t = -4.6991, df = 9.3736, p-value = 0.001004 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -7.966165 -2.809916 ## sample estimates: ## mean of x mean of y ## -0.2201914 5.1678489 # Run a standard, paired t-test t.test(x = before, y =after, paired = TRUE) ## ## Paired t-test ## ## data: before and after ## t = -5.138, df = 9, p-value = 0.0006129 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -7.760267 -3.015813 ## sample estimates: ## mean of the differences ## -5.38804 In this case, both the paired and the unpaired versions were significant, but the paired one showed a larger difference (more powerful). 6.3.2 Repeated Measures ANOVA here we are going to use this process but for repeated (more than 2) measures using ANOVA. First, though, we have to create a data frame using the variables from the last section. # Create the data.frame, using the variables from the previous exercise. # y is the joined vectors before and after. # trial is the repeated names before and after, each one repeated n_ind # ind is the letter of the individual, repeated 2 times (because there were two observations) dat &lt;- data.frame(y = c(before, after), trial = rep(c(&quot;before&quot;, &quot;after&quot;), each = 10), ind = rep(letters[1:10], times = 2)) library(lmerTest) ## Warning: package &#39;lmerTest&#39; was built under R version 3.5.3 ## ## Attaching package: &#39;lmerTest&#39; ## The following object is masked from &#39;package:lme4&#39;: ## ## lmer ## The following object is masked from &#39;package:stats&#39;: ## ## step # Run a standard, paired t-test t.test(before, after, paired = TRUE) ## ## Paired t-test ## ## data: before and after ## t = -5.138, df = 9, p-value = 0.0006129 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -7.760267 -3.015813 ## sample estimates: ## mean of the differences ## -5.38804 # Run a lmer and save it as lmer_out lmer_out &lt;- lmer(y ~ trial + (1| ind), data = dat) # Look at the summary() of lmer_out summary(lmer_out) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: y ~ trial + (1 | ind) ## Data: dat ## ## REML criterion at convergence: 89.3 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.89008 -0.30775 -0.02928 0.22414 2.41195 ## ## Random effects: ## Groups Name Variance Std.Dev. ## ind (Intercept) 1.075 1.037 ## Residual 5.498 2.345 ## Number of obs: 20, groups: ind, 10 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 5.1678 0.8108 17.5311 6.374 5.99e-06 *** ## trialbefore -5.3880 1.0487 9.0000 -5.138 0.000613 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## trialbefore -0.647 In this case, the intercept is the same for the lmer and the t.test! So the lmer is basically an extension fo the paired t.test. 6.3.3 Example: NY Hate Crime Data hate &lt;- read_csv(&quot;https://assets.datacamp.com/production/repositories/1803/datasets/45e88fe1bc8d1d76d140e69cb873da9eddb7008e/hateNY.csv&quot;) ## Parsed with column specification: ## cols( ## Year = col_double(), ## County = col_character(), ## TotalIncidents = col_double(), ## Year2 = col_double() ## ) hate ## # A tibble: 233 x 4 ## Year County TotalIncidents Year2 ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2010 Albany 13 0 ## 2 2011 Albany 7 1 ## 3 2012 Albany 5 2 ## 4 2013 Albany 3 3 ## 5 2014 Albany 3 4 ## 6 2015 Albany 3 5 ## 7 2016 Albany 3 6 ## 8 2013 Allegany 1 3 ## 9 2010 Bronx 22 0 ## 10 2011 Bronx 11 1 ## # ... with 223 more rows Give the different population sizes of New York counties, you can reasonably assume the need for random-effect intercepts a priori. However, do you need random-effect slopes? Plot the data to see if trends appear to vary by county. Additionally, plotting the data will help you see what is going on. ggplot(hate, aes(x = Year, y = TotalIncidents, group = County)) + geom_line() + geom_smooth(method = &quot;glm&quot;, method.args = c(&quot;poisson&quot;), se = FALSE) From the graph above, it looks like we need both random intercepts and random slopes. Now let’s build a simple glm model and then we cna move onto glmer. ny_hate_mod1 &lt;- glm(TotalIncidents ~ Year + County, data = hate, family = &quot;poisson&quot;) summary(ny_hate_mod1) ## ## Call: ## glm(formula = TotalIncidents ~ Year + County, family = &quot;poisson&quot;, ## data = hate) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.8488 -0.6322 -0.1068 0.4336 4.9002 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 121.98334 21.85207 5.582 2.37e-08 *** ## Year -0.05977 0.01086 -5.506 3.67e-08 *** ## CountyAllegany -1.65787 1.01343 -1.636 0.101859 ## CountyBronx 1.15126 0.18861 6.104 1.04e-09 *** ## CountyBroome -0.90255 0.32242 -2.799 0.005121 ** ## CountyCattaraugus -1.45353 0.47648 -3.051 0.002284 ** ## CountyCayuga -1.01203 0.44017 -2.299 0.021494 * ## CountyChautauqua -1.48607 0.47654 -3.118 0.001818 ** ## CountyChemung -1.70959 0.60037 -2.848 0.004405 ** ## CountyChenango -1.17785 0.47650 -2.472 0.013441 * ## CountyClinton -0.68000 0.37218 -1.827 0.067692 . ## CountyColumbia -1.72240 0.60038 -2.869 0.004120 ** ## CountyCortland -1.52662 0.47669 -3.203 0.001362 ** ## CountyDelaware -1.53832 1.01371 -1.518 0.129138 ## CountyDutchess 0.01178 0.23948 0.049 0.960756 ## CountyErie 1.50107 0.18180 8.257 &lt; 2e-16 *** ## CountyEssex -1.45495 0.52651 -2.763 0.005721 ** ## CountyFranklin -1.24769 0.44011 -2.835 0.004583 ** ## CountyFulton -1.71765 1.01346 -1.695 0.090106 . ## CountyGenesee -1.65787 1.01343 -1.636 0.101859 ## CountyGreene -1.53832 1.01371 -1.518 0.129138 ## CountyHerkimer -1.72478 0.72605 -2.376 0.017522 * ## CountyJefferson -1.61690 0.52642 -3.072 0.002130 ** ## CountyKings 2.56910 0.17058 15.061 &lt; 2e-16 *** ## CountyLewis -1.71765 1.01346 -1.695 0.090106 . ## CountyLivingston -1.22832 0.44015 -2.791 0.005259 ** ## CountyMadison -0.87154 0.41226 -2.114 0.034512 * ## CountyMonroe -0.20972 0.24568 -0.854 0.393315 ## CountyMultiple -1.09013 0.37168 -2.933 0.003357 ** ## CountyNassau 0.79580 0.19805 4.018 5.86e-05 *** ## CountyNew York 2.48491 0.17111 14.522 &lt; 2e-16 *** ## CountyNiagara -0.60248 0.30616 -1.968 0.049088 * ## CountyOneida -1.11226 0.47654 -2.334 0.019594 * ## CountyOnondaga -1.17683 0.41236 -2.854 0.004318 ** ## CountyOntario -1.76025 0.60053 -2.931 0.003377 ** ## CountyOrange -0.17693 0.24349 -0.727 0.467435 ## CountyOrleans -1.40229 0.60081 -2.334 0.019596 * ## CountyOswego -0.73686 0.35641 -2.067 0.038693 * ## CountyOtsego -0.89300 0.32241 -2.770 0.005610 ** ## CountyPutnam -0.99506 0.52634 -1.891 0.058689 . ## CountyQueens 1.51898 0.18151 8.369 &lt; 2e-16 *** ## CountyRensselaer -1.39772 0.52635 -2.656 0.007919 ** ## CountyRichmond 0.66575 0.20228 3.291 0.000997 *** ## CountyRockland 0.04996 0.23759 0.210 0.833466 ## CountySaratoga -0.83543 0.33230 -2.514 0.011934 * ## CountySchenectady -0.74492 0.39007 -1.910 0.056167 . ## CountySchoharie -1.58092 0.60052 -2.633 0.008473 ** ## CountySchuyler -1.77742 1.01360 -1.754 0.079505 . ## CountySeneca -1.71765 1.01346 -1.695 0.090106 . ## CountySt. Lawrence -1.04982 0.34342 -3.057 0.002236 ** ## CountySteuben -1.34608 0.60049 -2.242 0.024985 * ## CountySuffolk 1.45168 0.18264 7.948 1.89e-15 *** ## CountySullivan -1.83720 1.01387 -1.812 0.069976 . ## CountyTioga -1.59988 0.72608 -2.203 0.027563 * ## CountyTompkins -0.63539 0.31378 -2.025 0.042869 * ## CountyUlster -0.88945 0.34343 -2.590 0.009599 ** ## CountyWarren -1.53832 1.01371 -1.518 0.129138 ## CountyWashington -1.43472 0.52643 -2.725 0.006423 ** ## CountyWayne -1.11461 0.52691 -2.115 0.034400 * ## CountyWestchester 0.74295 0.19630 3.785 0.000154 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 4106.87 on 232 degrees of freedom ## Residual deviance: 302.27 on 173 degrees of freedom ## AIC: 1160.4 ## ## Number of Fisher Scoring iterations: 5 Now that our model ran without any problems, let’s build a glmer(): ny_hate_mod2 &lt;- glmer(TotalIncidents ~ Year + (Year | County), data = hate, family = &quot;poisson&quot;) ## Warning in checkConv(attr(opt, &quot;derivs&quot;), opt$par, ctrl = ## control$checkConv, : Model failed to converge with max|grad| = 0.370207 ## (tol = 0.001, component 1) ## Warning in checkConv(attr(opt, &quot;derivs&quot;), opt$par, ctrl = control$checkConv, : Model is nearly unidentifiable: very large eigenvalue ## - Rescale variables?;Model is nearly unidentifiable: large eigenvalue ratio ## - Rescale variables? summary(ny_hate_mod2) ## Generalized linear mixed model fit by maximum likelihood (Laplace ## Approximation) [glmerMod] ## Family: poisson ( log ) ## Formula: TotalIncidents ~ Year + (Year | County) ## Data: hate ## ## AIC BIC logLik deviance df.resid ## 1165.3 1182.5 -577.6 1155.3 228 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.5434 -0.4861 -0.1560 0.3330 3.1938 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## County (Intercept) 4.748e+04 217.8915 ## Year 1.175e-02 0.1084 -1.00 ## Number of obs: 233, groups: County, 59 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 295.481381 17.537537 16.85 &lt;2e-16 *** ## Year -0.146370 0.008717 -16.79 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## Year -1.000 ## convergence code: 0 ## Model failed to converge with max|grad| = 0.370207 (tol = 0.001, component 1) ## Model is nearly unidentifiable: very large eigenvalue ## - Rescale variables? ## Model is nearly unidentifiable: large eigenvalue ratio ## - Rescale variables? We got a wnrning about having a problem and needing to rescale, we can do this by using the Year2 variable instead of the Year variable (rescaling can help a lot with this) ny_hate_mod3 &lt;- glmer(TotalIncidents ~ Year2 + (Year2 | County), data = hate, family = &quot;poisson&quot;) summary(ny_hate_mod3) ## Generalized linear mixed model fit by maximum likelihood (Laplace ## Approximation) [glmerMod] ## Family: poisson ( log ) ## Formula: TotalIncidents ~ Year2 + (Year2 | County) ## Data: hate ## ## AIC BIC logLik deviance df.resid ## 1165.3 1182.5 -577.6 1155.3 228 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.5434 -0.4864 -0.1562 0.3319 3.1939 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## County (Intercept) 1.16291 1.0784 ## Year2 0.01175 0.1084 0.02 ## Number of obs: 233, groups: County, 59 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.27952 0.16600 7.708 1.28e-14 *** ## Year2 -0.14622 0.03324 -4.398 1.09e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## Year2 -0.338 Now let’s visualize the results: During this exercise, we’ll extract out the county-level estimates and plot them with ggplot2. The county-level random-effect slopes need to be added to the fixed-effect slopes to get the slope estimates for each county. In addition to this addition, the code includes ordering the counties by rate of crime (the slope estimates) to help visualize the data clearly. # Extract out the fixed-effect slope for Year2 Year2_slope &lt;- fixef(ny_hate_mod3)[&#39;Year2&#39;] # Extract out the random-effect slopes for county county_slope &lt;- ranef(ny_hate_mod3)$County # Create a new column for the slope county_slope$slope &lt;- county_slope$Year2 + Year2_slope # Use the row names to create a county name column county_slope$county &lt;- rownames(county_slope) # Create an ordered county-level factor based upon slope values county_slope$county_plot &lt;- factor(county_slope$county, levels = county_slope$county[order(county_slope$slope)]) # Now plot the results using ggplot2 ggplot(data = county_slope, aes(x = county_plot, y = slope)) + geom_point() + coord_flip() + theme_bw() + ylab(&quot;Change in hate crimes per year&quot;) + xlab(&quot;County&quot;) Note how the change in hate crimes varies greatly across countries! "],
["count-data.html", "Chapter 7 Count Data", " Chapter 7 Count Data A Poisson distribution is different than a normal distribution in that it only has discrete integers (positive and whole numbers). A Poisson distribution also have the assumption of equi-dispersion meaning that the mean and variance parameter are the same (somethign commonly broken leading to us using a negative binomial distribution). To run a poisson regression in R you need: Discrete counts Defined area and time As a reminder, our coefficients are on a log-scale. The formula is a pretty simple variation of the glm() framework: glm(y ~ x, data = dat, family = 'poisson') When you shouldn’t use poisson distrubtion: Non-count or non-positive data (e.g. 1.4 or -2) Non-constant sample area or time Mean &gt; 30 (can probably use a normal distrubition) Over-dispersed data Zero-inflated data "],
["lets-fit-the-first-one.html", "Chapter 8 let’s fit the first one:", " Chapter 8 let’s fit the first one: This dataset is coming from Louisville data on civilian fire victim data. First we will run a linear model then compare it with the count data. This data is appropriate for count models because it is a whole number (people) and can’t ever be negative. library(tidyverse) library(broom) fire &lt;- read_csv(&quot;Data/count_fire_data.csv&quot;) %&gt;% mutate(month = as.factor(month)) ## Parsed with column specification: ## cols( ## ID = col_double(), ## Date = col_character(), ## victims = col_double(), ## month = col_double() ## ) ## Warning: 1 parsing failure. ## row col expected actual file ## 4369 ID no trailing characters 2016-12-25 1 12 &#39;Data/count_fire_data.csv&#39; linear &lt;- lm(victims ~ month, data = fire) count &lt;- glm(victims ~ month, data = fire, family = &quot;poisson&quot;) summary(linear) ## ## Call: ## lm(formula = victims ~ month, data = fire) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.1295 -0.1056 -0.0914 -0.0753 8.8763 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.129477 0.022770 5.686 1.38e-08 *** ## month2 -0.038031 0.032767 -1.161 0.2458 ## month3 -0.078401 0.032007 -2.450 0.0143 * ## month4 -0.057254 0.032269 -1.774 0.0761 . ## month5 -0.032702 0.032007 -1.022 0.3070 ## month6 -0.043365 0.032269 -1.344 0.1791 ## month7 -0.005821 0.032007 -0.182 0.8557 ## month8 -0.051520 0.032007 -1.610 0.1075 ## month9 -0.023921 0.032269 -0.741 0.4586 ## month10 -0.054208 0.032007 -1.694 0.0904 . ## month11 -0.023921 0.032269 -0.741 0.4586 ## month12 -0.022919 0.032136 -0.713 0.4758 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4338 on 4356 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.00249, Adjusted R-squared: -2.927e-05 ## F-statistic: 0.9884 on 11 and 4356 DF, p-value: 0.4542 summary(count) ## ## Call: ## glm(formula = victims ~ month, family = &quot;poisson&quot;, data = fire) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.5089 -0.4595 -0.4277 -0.3880 7.7086 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.0443 0.1459 -14.015 &lt; 2e-16 *** ## month2 -0.3478 0.2314 -1.503 0.132839 ## month3 -0.9302 0.2719 -3.422 0.000623 *** ## month4 -0.5837 0.2444 -2.388 0.016923 * ## month5 -0.2911 0.2215 -1.314 0.188706 ## month6 -0.4079 0.2314 -1.763 0.077939 . ## month7 -0.0460 0.2074 -0.222 0.824486 ## month8 -0.5073 0.2361 -2.149 0.031671 * ## month9 -0.2043 0.2182 -0.936 0.349112 ## month10 -0.5424 0.2387 -2.272 0.023075 * ## month11 -0.2043 0.2182 -0.936 0.349112 ## month12 -0.1948 0.2166 -0.899 0.368434 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 2325.3 on 4367 degrees of freedom ## Residual deviance: 2302.7 on 4356 degrees of freedom ## (1 observation deleted due to missingness) ## AIC: 2975.6 ## ## Number of Fisher Scoring iterations: 6 Note how the count model is better able to determine which months were significant. "]
]
